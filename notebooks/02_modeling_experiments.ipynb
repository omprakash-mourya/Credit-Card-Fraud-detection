{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0037dde4",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Modeling Experiments\n",
    "\n",
    "This notebook contains modeling experiments including baseline models, SMOTE experiments, XGBoost tuning, threshold optimization, and model explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfcab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from src.data_utils import load_data\n",
    "from src.preprocess import fit_transform_pipeline\n",
    "from src.train import train_baseline_logistic, train_xgboost_with_smote\n",
    "from src.evaluate import (\n",
    "    evaluate_model, plot_roc_curve, plot_precision_recall_curve,\n",
    "    threshold_tuning, plot_threshold_analysis\n",
    ")\n",
    "from src.explain import explain_shap, plot_feature_importance\n",
    "from src.config import RANDOM_STATE, TEST_SIZE, FEATURE_COLUMNS\n",
    "from src.utils import set_seed\n",
    "\n",
    "# Set style and seed\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde785c9",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "try:\n",
    "    df = load_data()\n",
    "    print(f\"Data loaded: {df.shape}\")\n",
    "    \n",
    "    # Fit preprocessing pipeline\n",
    "    pipeline, X, y = fit_transform_pipeline(df)\n",
    "    print(f\"Features processed: {X.shape}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    print(f\"Train fraud rate: {y_train.mean():.4f}\")\n",
    "    print(f\"Test fraud rate: {y_test.mean():.4f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please download creditcard.csv and place it in data/creditcard.csv\")\n",
    "    print(\"You can run the training script instead: python -m src.train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99a128",
   "metadata": {},
   "source": [
    "## 2. Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals():\n",
    "    print(\"Training baseline Logistic Regression...\")\n",
    "    \n",
    "    # Train baseline model\n",
    "    lr_model, lr_metrics = train_baseline_logistic(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Logistic Regression Results:\")\n",
    "    print(f\"ROC-AUC: {lr_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"PR-AUC: {lr_metrics['pr_auc']:.4f}\")\n",
    "    print(f\"Precision: {lr_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {lr_metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {lr_metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(lr_metrics['confusion_matrix'])\n",
    "else:\n",
    "    print(\"Data not available. Please load the dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b60b18",
   "metadata": {},
   "source": [
    "## 3. XGBoost with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals():\n",
    "    print(\"Training XGBoost with SMOTE...\")\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    xgb_model, xgb_metrics = train_xgboost_with_smote(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"\\nðŸš€ XGBoost Results:\")\n",
    "    print(f\"ROC-AUC: {xgb_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"PR-AUC: {xgb_metrics['pr_auc']:.4f}\")\n",
    "    print(f\"Precision: {xgb_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {xgb_metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {xgb_metrics['f1_score']:.4f}\")\n",
    "    print(f\"Best CV Score: {xgb_metrics['best_cv_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nBest Parameters:\")\n",
    "    for param, value in xgb_metrics['best_params'].items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(xgb_metrics['confusion_matrix'])\n",
    "else:\n",
    "    print(\"Data not available. Please load the dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d48834",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'lr_metrics' in locals() and 'xgb_metrics' in locals():\n",
    "    # Create comparison table\n",
    "    comparison_data = {\n",
    "        'Metric': ['ROC-AUC', 'PR-AUC', 'Precision', 'Recall', 'F1-Score'],\n",
    "        'Logistic Regression': [\n",
    "            lr_metrics['roc_auc'],\n",
    "            lr_metrics['pr_auc'],\n",
    "            lr_metrics['precision'],\n",
    "            lr_metrics['recall'],\n",
    "            lr_metrics['f1_score']\n",
    "        ],\n",
    "        'XGBoost + SMOTE': [\n",
    "            xgb_metrics['roc_auc'],\n",
    "            xgb_metrics['pr_auc'],\n",
    "            xgb_metrics['precision'],\n",
    "            xgb_metrics['recall'],\n",
    "            xgb_metrics['f1_score']\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    print(\"ðŸ“Š Model Comparison:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(len(comparison_df['Metric']))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, comparison_df['Logistic Regression'], width, \n",
    "           label='Logistic Regression', alpha=0.7)\n",
    "    ax.bar(x + width/2, comparison_df['XGBoost + SMOTE'], width, \n",
    "           label='XGBoost + SMOTE', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(comparison_df['Metric'])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Models not trained yet. Please run the previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845064b8",
   "metadata": {},
   "source": [
    "## 5. ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_model' in locals():\n",
    "    # Plot ROC curve\n",
    "    fig1 = plot_roc_curve(xgb_model, X_test, y_test)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    fig2 = plot_precision_recall_curve(xgb_model, X_test, y_test)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"XGBoost model not available. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eddbdb",
   "metadata": {},
   "source": [
    "## 6. Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_model' in locals():\n",
    "    print(\"Performing threshold tuning...\")\n",
    "    \n",
    "    # Get probabilities\n",
    "    y_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Tune threshold\n",
    "    optimal_threshold, threshold_results = threshold_tuning(\n",
    "        y_proba, y_test, cost_fp=1, cost_fn=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"Optimal Cost: {threshold_results['optimal_cost']:.2f}\")\n",
    "    \n",
    "    # Plot threshold analysis\n",
    "    fig = plot_threshold_analysis(threshold_results)\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance at optimal threshold\n",
    "    y_pred_optimal = (y_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Performance at Optimal Threshold:\")\n",
    "    print(classification_report(y_test, y_pred_optimal, target_names=['Normal', 'Fraud']))\n",
    "    \n",
    "    # Confusion matrix at optimal threshold\n",
    "    cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(f'Confusion Matrix (Threshold = {optimal_threshold:.3f})')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"XGBoost model not available. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb558f6",
   "metadata": {},
   "source": [
    "## 7. Feature Importance and SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_model' in locals():\n",
    "    print(\"Generating SHAP explanations...\")\n",
    "    \n",
    "    # Get sample for SHAP analysis\n",
    "    sample_size = 1000\n",
    "    sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "    X_sample = X_test[sample_indices]\n",
    "    \n",
    "    # Generate SHAP explanations\n",
    "    shap_results = explain_shap(xgb_model, X_sample, FEATURE_COLUMNS)\n",
    "    \n",
    "    if shap_results:\n",
    "        print(\"âœ… SHAP analysis completed!\")\n",
    "        \n",
    "        # Plot feature importance from SHAP\n",
    "        if 'feature_importance' in shap_results:\n",
    "            fig = plot_feature_importance(\n",
    "                shap_results['feature_importance'], \n",
    "                title=\"SHAP Feature Importance\",\n",
    "                top_n=15\n",
    "            )\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\nðŸ” Top 10 Most Important Features (SHAP):\")\n",
    "        if 'feature_importance' in shap_results:\n",
    "            sorted_features = sorted(\n",
    "                shap_results['feature_importance'].items(), \n",
    "                key=lambda x: abs(x[1]), \n",
    "                reverse=True\n",
    "            )\n",
    "            for i, (feature, importance) in enumerate(sorted_features[:10], 1):\n",
    "                print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ SHAP analysis failed. This might be due to missing SHAP library.\")\n",
    "        print(\"Install with: pip install shap\")\n",
    "        \n",
    "        # Fallback to XGBoost feature importance\n",
    "        print(\"\\nUsing XGBoost built-in feature importance instead:\")\n",
    "        feature_importance = dict(zip(FEATURE_COLUMNS, xgb_model.feature_importances_))\n",
    "        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        fig = plot_feature_importance(feature_importance, title=\"XGBoost Feature Importance\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nðŸ” Top 10 Most Important Features (XGBoost):\")\n",
    "        for i, (feature, importance) in enumerate(sorted_features[:10], 1):\n",
    "            print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"XGBoost model not available. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfef437",
   "metadata": {},
   "source": [
    "## 8. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75670e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_model' in locals() and 'optimal_threshold' in locals():\n",
    "    print(\"ðŸ’¼ Business Impact Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Calculate business metrics\n",
    "    y_pred_default = xgb_model.predict(X_test)  # Default threshold (0.5)\n",
    "    y_pred_optimal = (y_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    cm_default = confusion_matrix(y_test, y_pred_default)\n",
    "    cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "    \n",
    "    def calculate_costs(cm, cost_fp=1, cost_fn=5):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        total_cost = fp * cost_fp + fn * cost_fn\n",
    "        return tn, fp, fn, tp, total_cost\n",
    "    \n",
    "    # Default threshold analysis\n",
    "    tn_def, fp_def, fn_def, tp_def, cost_def = calculate_costs(cm_default)\n",
    "    \n",
    "    # Optimal threshold analysis\n",
    "    tn_opt, fp_opt, fn_opt, tp_opt, cost_opt = calculate_costs(cm_optimal)\n",
    "    \n",
    "    print(f\"ðŸ“Š Threshold Comparison:\")\n",
    "    print(f\"\\nDefault Threshold (0.5):\")\n",
    "    print(f\"  â€¢ True Positives (Caught Fraud): {tp_def}\")\n",
    "    print(f\"  â€¢ False Positives (False Alarms): {fp_def}\")\n",
    "    print(f\"  â€¢ False Negatives (Missed Fraud): {fn_def}\")\n",
    "    print(f\"  â€¢ Total Cost: {cost_def}\")\n",
    "    print(f\"  â€¢ Fraud Detection Rate: {tp_def/(tp_def + fn_def):.1%}\")\n",
    "    \n",
    "    print(f\"\\nOptimal Threshold ({optimal_threshold:.3f}):\")\n",
    "    print(f\"  â€¢ True Positives (Caught Fraud): {tp_opt}\")\n",
    "    print(f\"  â€¢ False Positives (False Alarms): {fp_opt}\")\n",
    "    print(f\"  â€¢ False Negatives (Missed Fraud): {fn_opt}\")\n",
    "    print(f\"  â€¢ Total Cost: {cost_opt}\")\n",
    "    print(f\"  â€¢ Fraud Detection Rate: {tp_opt/(tp_opt + fn_opt):.1%}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’° Cost Savings: {cost_def - cost_opt} units ({(cost_def - cost_opt)/cost_def:.1%} reduction)\")\n",
    "    \n",
    "    # Additional insights\n",
    "    fraud_amount = df[df['Class'] == 1]['Amount'].sum()\n",
    "    avg_fraud_amount = df[df['Class'] == 1]['Amount'].mean()\n",
    "    \n",
    "    print(f\"\\nðŸ’³ Financial Impact (from training data):\")\n",
    "    print(f\"  â€¢ Total fraud amount: ${fraud_amount:,.2f}\")\n",
    "    print(f\"  â€¢ Average fraud amount: ${avg_fraud_amount:.2f}\")\n",
    "    print(f\"  â€¢ Potential fraud caught (optimal): ${tp_opt * avg_fraud_amount:,.2f}\")\n",
    "    print(f\"  â€¢ Potential fraud missed (optimal): ${fn_opt * avg_fraud_amount:,.2f}\")\n",
    "else:\n",
    "    print(\"Models and threshold analysis not available. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b9c388",
   "metadata": {},
   "source": [
    "## 9. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_metrics' in locals():\n",
    "    print(\"ðŸŽ¯ MODELING SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Best Model Performance:\")\n",
    "    print(f\"  â€¢ Model: XGBoost with SMOTE\")\n",
    "    print(f\"  â€¢ ROC-AUC: {xgb_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  â€¢ PR-AUC: {xgb_metrics['pr_auc']:.4f}\")\n",
    "    print(f\"  â€¢ Recall: {xgb_metrics['recall']:.4f} (fraud detection rate)\")\n",
    "    \n",
    "    if 'optimal_threshold' in locals():\n",
    "        print(f\"  â€¢ Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Key Modeling Decisions:\")\n",
    "    print(f\"  â€¢ SMOTE oversampling to handle class imbalance\")\n",
    "    print(f\"  â€¢ XGBoost for complex pattern detection\")\n",
    "    print(f\"  â€¢ RandomizedSearchCV for hyperparameter tuning\")\n",
    "    print(f\"  â€¢ Cost-based threshold optimization\")\n",
    "    print(f\"  â€¢ Focus on recall to minimize missed fraud\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Business Recommendations:\")\n",
    "    print(f\"  â€¢ Deploy XGBoost model with optimal threshold\")\n",
    "    print(f\"  â€¢ Monitor model performance regularly\")\n",
    "    print(f\"  â€¢ Implement real-time scoring for transactions\")\n",
    "    print(f\"  â€¢ Use SHAP explanations for model interpretability\")\n",
    "    print(f\"  â€¢ Consider ensemble methods for further improvement\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ Next Steps:\")\n",
    "    print(f\"  â€¢ A/B testing with current fraud detection system\")\n",
    "    print(f\"  â€¢ Continuous model retraining with new data\")\n",
    "    print(f\"  â€¢ Feature engineering based on domain expertise\")\n",
    "    print(f\"  â€¢ Integration with transaction processing system\")\n",
    "    print(f\"  â€¢ Regular model validation and monitoring\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âœ… Modeling experiments completed successfully!\")\n",
    "else:\n",
    "    print(\"Please run the modeling experiments first to see the summary.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
